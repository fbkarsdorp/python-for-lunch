{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/python-lunch.jpg\" align=\"right\" width=100/><h1>Python for Lunch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Folgert Karsdorp](https://github.com/fbkarsdorp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"There ain't no such thing as a free lunch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone can learn how to program and the best way to learn is by doing. During this lunch lecture you will be asked to write a lot of code. Click any block of code in this tutorial, such as the one above, and press `ctrl+enter` to run it. Let's begin right away and write our first little program! Have a look at the following code block, run it, and try to understand what is happening. \n",
    "\n",
    "- Try to replace some or all of the words with different ones to print a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word1 = \"Python\"\n",
    "word2 = \"for\"\n",
    "word3 = \"Lunch\"\n",
    "\n",
    "print(word1 + \" \" + word2 + \" \" + word3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code block, we made use of so-called variables. Variables can thought of as kind of container in which you can store all kinds of information. In the previous example we stored the string `\"Python\"` into the variable `word1`. Although they look similar, there is an important difference between strings such as `\"Python\"` and variables such as `word2`. As you can see strings are surrounded by quotation marks, whereas variables are not. Without those quotes Python thinks its dealing with variables. To make the difference more clear, have a look at the following lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Python = \"Lunch\"\n",
    "Lunch = \"Python\"\n",
    "\n",
    "print(Python + \" \" + word2 + \" \" + Lunch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows that variable names can be chosen arbitrarily. *We* give a certain value a name, and we are free to pick one to our liking. It is, however, recommended to use senseful names because it helps you to remember what you stored in a variable. \n",
    "\n",
    "Variables are not just strings. They can also be numbers or other more complicated objects which we'll see in a moment. A more general term for the objects we store into variables is *value*. If you read a programming book, people generally speak about storing, or assigning values to variables. Have a look at the following code block. \n",
    "\n",
    "- Can you add a line at the bottom that prints the addition of `small_number` and `large_number`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_number = 1\n",
    "large_number = 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even more tricky: can you store the result of adding these numbers into a new variable called `even_larger_number`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove this line and insert your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This very basic concept of variables and values lies at the heart of almost all computer code. Variables allow you to store information for later reuse and manipulation, to which we will now turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for Data Carpentry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/messydata.jpeg\" align=\"right\" width=400/>For scholars in the Humanities, text cleaning is probably one of the most important and most labour intensive activities. A lot of data collections we work with come from old manuscripts, websites, books and often in a format that is not easily readable. Moreover, the data is often highly messy and needs cleansing before we can proceed with our analyses. Python can be an immense help in both obtaining and cleaning data for further research. I will try to show some of the basic steps you need to take to obtain and clean your data. We will work with a nice corpus of chain letters which can be found [here](http://www.silcom.com/~barnowl/chain-letter/archive/%21content.html). \n",
    "\n",
    "If you want to obtain this corpus without the help of Python (or some other programming language), you probably need to click on each of the letters, copy the contents of the page and manually save that in a file. As the archive consists of over 900 letters, this will probably take you a day, if not more. Once you have some basic understanding of Python, you could write a script to do all this tedious and boring work for you. Let's start with downloading a single letter. The following code block contains a lot of new stuff that you haven't seen yet. Still, you should be able to figure out what is going on here. Have a go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://www.silcom.com/~barnowl/chain-letter/archive/le1995-08_dl-_wk.htm\"\n",
    "webpage = requests.get(url)\n",
    "\n",
    "print(webpage.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is an open source, and very popular programming language. Many people have written programs, modules,and packages for Python that are freely available, often on platforms such as [Github](https://github.com/). One example is the `requests` library which we used in this example. It comes preinstalled with the Anaconda Python distribution and you can import those libraries using the statement `import name-of-the-library-you-want-to-import`. If you're using a different distribution of Python, you can can download the library from [here](http://www.python-requests.org/en/latest/).\n",
    "\n",
    "Now that we have downloaded our first letter, we would like to remove all HTML markup. Of course, automatically. The library [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/) is yet another wonderful example of freely available and open source software. We will use that library to extract all the text from the HTML page. Again, just try to read the code, execute the cell, and try figure out what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "souped = bs4.BeautifulSoup(webpage.text)\n",
    "\n",
    "print(souped.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only left is to write some code that will automatically download all the letters we're interested in. In this brief tutorial I don't have the time to fully explain how you can do that. I want to download all chain letters of the *death lottery* type. The manual of the website explains that these letters contain the tag `_dl_` in their url. I also only want to extract the chain letters written in English. This is marked with the tag `le`. The following code block contains all the code you need to download all the pages, extract the text and store them into a variable called `death_lottery_pages`. Have a look. Don't be scared. Just try to figure out what the different parts probably try to achieve. \n",
    "\n",
    "**A little warning**: it may take some time to download all the pages. So once you've executed the cell below, please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retrieve the archive page, listing all the links to the chain letters\n",
    "archive = requests.get(\"http://www.silcom.com/~barnowl/chain-letter/archive/%21content.html\")\n",
    "# extract all HTML from the archive\n",
    "souped = bs4.BeautifulSoup(archive.text)\n",
    "\n",
    "# create a variable called death_lotter_link in which we store all links found on the archive page\n",
    "death_lottery_links = []\n",
    "# Now iterate (or loop) over all html elements in the page, we're only interested in <a href\"\"> \n",
    "# elements, which is why I use the find_all function\n",
    "for link in souped.find_all('a', href=True):\n",
    "    # if a link contains the string \"_dl_\" and starts with the string \"le\"\n",
    "    if '_dl_' in link['href'] and link['href'].startswith('le'):\n",
    "        # then add (append) it to the death lottery links\n",
    "        death_lottery_links.append(link['href'])\n",
    "\n",
    "\n",
    "# now that we have an ordered list of links, we can iterate over that list and \n",
    "# download and extract all the content of those pages.\n",
    "\n",
    "# first declare the base_url of the archive\n",
    "base_url = \"http://www.silcom.com/~barnowl/chain-letter/archive/\"\n",
    "# next, we declare a variable called death_lottery_pages in which we store all contents of the web pages.\n",
    "death_lottery_pages = []\n",
    "# we iterate over all links stored in the variable death_lottery_links\n",
    "for link in death_lottery_links:\n",
    "    # these links are relative links. To download them, we need the full link. \n",
    "    # So we concatenate the base_url with the link\n",
    "    fullink = base_url + link\n",
    "    # this part you know: retrieve the webpage associated with the link\n",
    "    webpage = requests.get(fullink)\n",
    "    # next \"soupify\" the webpage\n",
    "    souped_page = bs4.BeautifulSoup(webpage.text)\n",
    "    # Remove the HTML markup and append the text to the variable death_lottery pages\n",
    "    death_lottery_pages.append(souped_page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `death_lottery_pages` now holds 158 chain letters. You can check that using the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(death_lottery_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `death_lottery_pages` is called a `list`, which is an orderded sequence of objects. You can print a single letter by indexing this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(death_lottery_pages[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to change the number in the code above to print a different letter. What happens if you put in the number 158? What happens if you put in the number 0?\n",
    "\n",
    "As you can see, the letters still contain a lot of whitespace that we would rather remove. Python has a lot of functionality to work with text. A very convenient function is called `strip`, which allows you to remove unwanted whitespace from a string. Let's see how that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(death_lottery_pages[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's already a lot better, but we're not quite there yet. There are many sophisticated tokenizers available, but these are not included with the Anaconda distribution. An example is the [NLTK tokenizer](http://www.nltk.org/) or the [tokenizer](http://nlp.stanford.edu/software/) developed at Stanford University. For now we will make use of a fairly simple tokenizer that is a little rough on the edges, but it will do. We import and use the tokenizer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from simple_tokenizer import tokenize\n",
    "\n",
    "tokenize(death_lottery_pages[1], lowercase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the argument `lowercase=False` to `lowercase=True` and see what happens.\n",
    "\n",
    "Finally, we create a tokenized version of our chain letter corpus using the following lines of code. Execute the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "death_lottery_corpus = []\n",
    "for letter in death_lottery_pages:\n",
    "    death_lottery_corpus.append(tokenize(letter, lowercase=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dataanalysis.jpg\" align=\"left\" width=400/> We now proceed with a brief introduction into data analysis with Python. There are again many excellent libraries available for doing data analysis. The most sophisticated data analysis library is the Python Data Analysis Library, or Pandas. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. It comes pre-installed with the Ananconda Python distribution. \n",
    "\n",
    "I would like to show some basic exploratory techniques that help you to get a better idea of the data your working with. In the previous section we created a corpus of chain letters. Let's try to obtain some general statistics from this corpus. First we print the number of letters in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(death_lottery_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next it would be interesting to know the average word token length of the letters. We start with initializing a variable in which we sum the number of words per letter. We then divide that number by the number of letters in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_count = 0\n",
    "for letter in death_lottery_corpus:\n",
    "    for word in letter:\n",
    "        token_count += 1\n",
    "        \n",
    "print(token_count / len(death_lottery_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The build-in library `statistics` let's you compute a number of basic statistics in a much more convenient way. Let's first create a list in which we store the length of each letter in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_lengths = []\n",
    "for letter in death_lottery_corpus:\n",
    "    letter_lengths.append(len(letter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this list to compute the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "mean(letter_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statistics import variance\n",
    "\n",
    "variance(letter_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statistics import stdev\n",
    "\n",
    "stdev(letter_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a better understanding of the distribution we're dealing with, it's a good idea to plot the distribution using a histogram or some density plot. Again, just show some of the cool stuff that is included in the Anaconda distribution, we make use of another wonderful library called [seaborn](http://stanford.edu/~mwaskom/software/seaborn/). We first need to import the library. \n",
    "\n",
    "**Before you do that, please execute the following line which will make sure that the plots are shown in the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "seaborn.distplot(letter_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, ain't that a pretty normal distribution! Now let's get a little more sophisticated. The corpus we're dealing with is a chain letter corpus. Therefore, we could naively assume that most letters are very much alike. The letters are chronologically ordered on the website, and we downloaded them in that order. This allows to conveniently plot the number of new word types we attest with every new letter we observe through time. Execute the following cell to see a list of all the urls we downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "death_lottery_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My initial hypothesis would be that after seeing the first ten letters, the \"new word\" curve will reach its asymptote. Let's find out if that hypothesis holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a variable called words_seen which we be a set of words. \n",
    "# By definition a set only consists of unique items\n",
    "words_seen = set()\n",
    "# Initialize a list of the number of new words we've seen, starting with the number 0.\n",
    "unique_words_so_far = [0]\n",
    "# now, iterate over all letters in the death_lottery_corpus\n",
    "for letter in death_lottery_corpus:\n",
    "    # then iterate over each word in a letter\n",
    "    for word in letter:\n",
    "        # if that word is not yet in the set words_seen\n",
    "        if word not in words_seen:\n",
    "            # add it to the set\n",
    "            words_seen.add(word)\n",
    "    # once we've checked all words in a letter, compute the number of new words we've seen so far.\n",
    "    # this is simply the length of the set words_seen.\n",
    "    unique_words_so_far.append(len(words_seen))\n",
    "\n",
    "# now plot the curve\n",
    "seaborn.plt.plot(unique_words_so_far)\n",
    "# add a nice label on the x-axis\n",
    "seaborn.plt.xlabel(\"Chronologically ordered letters.\")\n",
    "# add a nice label on the y-axis\n",
    "seaborn.plt.ylabel(\"Number of unique words seen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can observe that the number of new word types diminishes over time, the effect is definitely not as strong as I would have thought. Although we must have closer look at the data, this curve might suggest that the letters have changed quite a bit over time. Let's have look at some parts of the first and the last letter in the corpus. Note that these fragments are rather unreadable, because we removed all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(' '.join(death_lottery_corpus[0][30:150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(' '.join(death_lottery_corpus[-1][42:150]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, although the letters bear many similarities there are also some interesting differences. In the first letter, we read about a general names Walsh who lost *life*. In the last letter the general has become Genio who lost his *wife*. In the next section we will we explore the corpus a little further with the help of some basic data visualization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/data-visualization.jpg\" align=\"right\" width=400/> Python has many options for data visualization. We've already seen one interesting library, Seaborn, but there are many others. The backbone of many of these libraries is the acclaimed plotting library [Matplotlib](http://matplotlib.org/). Matplotlib provides almost all functionality to make the fancy graphs you always dreamed of. The default color schemes of this library, however, don't look really pretty ([although people are working on this](http://bids.github.io/colormap/)). I therefore generally resort to Seaborn or to the interactive plotting library Bokeh. \n",
    "\n",
    "We will now have a brief look at some of the more advanced techniques and libraries you can use for doing text analysis. One particular technique I like for exploratory data analysis is clustering. Clustering is an unsupervised Machine Learning technique that enables you to partition you data into $n$ clusters. I know, it's getting boring, but yes, there is an excellent Machine Learning library available for Python. And by excellent, I really mean **excellent**. The library is called [scikit-learn](http://scikit-learn.org/stable/) and comes pre-installed with the Anaconda Python distribution.\n",
    "\n",
    "If we want to cluster texts, what kind of features can we use? A simple, yet often highly effective technique is to simply make use of word counts. For each word in a letter we compute how often that word occurs in that letter. The result of this can be represented by a vector of word counts:\n",
    "\n",
    "$$\\mathbf{\\vec{a}} = (w_1, w_2, \\ldots, w_n)$$\n",
    "\n",
    "where $w_i$ represents the count of word $i$ and $n$ the vocabulary size of the corpus. This vector can be compared to other word count vectors. And we could compute the distance of similarity between the vectors.\n",
    "\n",
    "In the Machine Learning literature and in the Information Retrieval literature as well, people tend to use word frequencies weighted by their document frequency in the corpus. The intuition behind this is that words that occur in many or all letters, such as function words, are not particularly interesting. By weighting the frequencies by their document frequencies, we promote words that occur often in a particular letter but not in the entire corpus. This weighting method is called *term-frequency inverse-document-frequency* or tf-idf for short. It is computed as follows:\n",
    "\n",
    "$$\\text{tf-idf}(w_i) = \\text{tf}(w_i) \\cdot \\text{idf}(w_i)$$ \n",
    "\n",
    "where $\\text{tf}(w_i)$ represents the count of word $i$ in document $d$ and $\\text{idf}(w_i)$ the inverse document frequency of word $i, which is computed as follows:\n",
    "\n",
    "$$\\text{idf}(w_i) = \\log \\frac{N}{|\\{d \\in D, w_i \\in d\\}|}$$\n",
    "\n",
    "Using the scikit-learn library, we can compute these frequencies in a convenient way. We first initialize a vectorizer, which we then use to `transform` our lottery pages into a vectorized object. Note that I'm reusing the variable `death_lottery_pages` and not our tokenized corpus. This is because the scikit-learn library features its own internal tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(death_lottery_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this vectorizer object as input to a cluster analysis. There are many different kinds of cluster analyses. Here I make use of a common technique that is called $k$-means clustering. First we import the relevant module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize a clustering object and cluster our data into 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters=5, init='k-means++')\n",
    "clusters = clusterer.fit_predict(X.toarray())\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in the array above represents the cluster to which a particular letter has been assigned to. Remember that our data is still chronologically ordered. We can use that information to print a slightly more informative list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for link, cluster in zip(death_lottery_links, clusters):\n",
    "    print(\"Cluster:\", cluster, \"Link:\", link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downside of the $k$-means clustering is that the researcher needs to manually set the number of clusters. Try to experiment with setting `n_clusters` to a higher and to a lower number and see what happens. \n",
    "\n",
    "This section is called data visualization, but we have seen a single visualization yet. We would like to somehow draw a map that visualizes the distances between all letters in our corpus. But think about it. Each letter is represented by a vector of about 2000 features (i.e. the words). To visualize this, we would need a 2000 dimensional plot, which is really hard if not impossible to grasp. Researchers have developed various dimension reduction techniques. One very popular and very powerful technique is called *Principle Component Analysis*, or PCA for short. I won't go into details, but this technique allows you to reduce the 2000 dimensions into 2 dimensions (one for x-axis and one for the y-axis) in which the distances between the letters are still present. \n",
    "\n",
    "We transform our data into a 2-dimensional version using the PCA class of scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "print(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this transformation, each letter will be represented by two numbers, one for x-axis and one for the y-axis. This allows us to plot the data on a map as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = X_pca[:,0], X_pca[:,1]\n",
    "seaborn.plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the PCA analysis is completely independent from the $k$-means analysis. The PCA analysis suggests that there are three or maybe four clear clusters in the data. Please rerun the $k$-means analysis, this time with `n_clusters=3`. \n",
    "\n",
    "The scatter plot above gives insight into how many clusters we can expect in our data. However, it's still not really informative because we cannot see which letter belongs to which cluster. We will therefore make use of a different visualization library, that allows us to interactively explore the clusters and the letters they consist of. We make use of the aforementioned Bokeh library. First we tell Bokeh to output the plots to the notebook we're working in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we create a figure and produce a scatter plot, using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "\n",
    "p = figure()\n",
    "p.circle(x, y)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the links of the letters by adding the line:\n",
    "\n",
    "    p.text(x, y, text= death_lottery_links, text_color=\"#333333\", \n",
    "           text_align=\"center\", text_font_size=\"10pt\")\n",
    "           \n",
    "just below `p.circle(x, y)` and above `show(p)`. Try that.\n",
    "\n",
    "Another way of adding the labels that keeps the plot more readable, is to use hover labels. Execute the following code block and play around, and finally eat your lunch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.palettes import Spectral11\n",
    "\n",
    "\n",
    "colors = [Spectral11[i] for i in clusters]\n",
    "\n",
    "TOOLS= \"pan,wheel_zoom,reset,hover,box_select,save\"\n",
    "source = ColumnDataSource(data=dict(x=x, y=y, link=death_lottery_links))\n",
    "\n",
    "p = figure(tools=TOOLS)\n",
    "p.circle(x, y, source=source, size=8, color=colors, fill_alpha=0.9, line_color=None)\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = [(\"link\", \"@link\")]\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've reached the end of the tutorial. Ignore the code below, it's just here to make the page a little prettier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "/*\n",
       "Placeholder for custom user CSS\n",
       "\n",
       "mainly to be overridden in profile/static/custom/custom.css\n",
       "\n",
       "This will always be an empty file in IPython\n",
       "*/\n",
       "<style>\n",
       "\n",
       "    @import url(http://fonts.googleapis.com/css?family=Inconsolata);\n",
       "\n",
       "    div.cell{\n",
       "        font-family:'Lucida Grande','helvetica','sans';\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Lucida Grande','helvetica','sans';\n",
       "        line-height: 145%;\n",
       "        font-size: 100%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "        font-family: \"Inconsolata\", source-code-pro, Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Python for Lunch</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"https://github.com/fbkarsdorp/python-for-lunch\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">https://github.com/fbkarsdorp/python-for-lunch</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>. Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/fbkarsdorp/python-for-lunch\" rel=\"dct:source\">https://github.com/fbkarsdorp/python-for-lunch</a>.</small></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
